
OK.
So going back to what I said, there's a bunch of words
that you've heard that are sort of related.
You're probably not here because you
want to do 1970s statistics.
You want to do machine learning.
You want to do data science.
You want to do artificial intelligence.
You want to do maybe data mining or analytics,
if you're more business driven.
And so all those are words that sort of
are all in the same nexus.
They all mean slightly different things.
There's entire sessions at statistics conferences
dedicated to defining those terms.
So it's not like we actually have the final answer to this.
There's sort of some organization that's coming up.
And I just wanted to give you an idea of what's the difference.
So this is probably not the right question,
because the answer is not going to tell you
what the difference is.
But I think all these things, what their similarity is,
is that they're all data driven fields.
Right?
So they're trying to gather data to get
some insight about what's happening
and ultimately make decisions.
And if you actually end up working as a statistician,
you'll see that it's not all-- so there's
a lot of machine learning, which is make some black box
decision about whether something is spam or not spam,
because no human being would ever want to look at this data.
Just want to get rid of this particular task
and get a computer to do it for them.
But if you work--
I work with biologists.
Biologists don't want a machine learning
algorithm black box that comes out and say, oh yes, this gene,
yes.
This gene, no.
What biologists want is for me to tell them
why, to allow them to look and peek inside the data
and have a biological story to tell to understand
what biology is going on.
And there's different ways of doing this.
And depending on where on the spectrum from statistics,
data science, machine learning you are,
you're going to have more and more opaque boxes
you're going to work with.
But at the end of the day, it's essentially
the same principles.
OK, so gather data, use data together insight
and ultimately make decision.
Machine learning, sometimes you skip the insight part
altogether.
No need to understand how it's working.
It just works.
So statistics is really the core.
It's a set of techniques.
And this class is fundamental statistics.
And what we want to understand are the principles
that guide this data principle thing.
We were trying to not make mistakes
and to try to understand how to use this data,
and not just look at numbers and have some wild guesses,
to have some principled ways of making decisions based on data.
And as we're moving towards bigger and bigger data sets,
computational aspects play a more and more important role.
And I think this is something that
comes, that attaches itself to statistics,
and that makes a huge part of machine learning.
For example, if you take a machine learning class over
in course six, you will see that actually many
of the statistical insights are taught
through algorithmic insights.
Right?
So if you look at a machine learning method,
it will mostly be described as an algorithm.
But what this algorithm implements
is some statistical principle in an efficient way.
And so in this class, we'll not touch much
upon computational aspects.
You will see-- and I will point to you--
I was like, I will show you, this
is an important computational bottleneck
for statistical methods.
But I won't really tell you how to solve them.
So at this level, for example, you
can think of using pretty high level statistical software that
will take care of this for you.
If you have a gigantic data set, if you're a lead data
scientist for a hedge fund, eventually you're
probably going to have to open the hood
and actually build your own algorithms to go faster.
So just a quick distribution of the roles here.
So the computational view is typically,
you have a large data set.
You view this as a large sequence of numbers.
And you need to process them relatively fast.
So the things that show up, for example,
in this kind of questions are, for example,
approximate nearest neighbors.
So MIT is extremely strong in that.
So for example, if you want to do some personalized
recommendations, you might look at what people
like you have done in the past.
So you need to understand what your neighbors are,
in the sense people have the same profile as you.
If your profile is 100,000 numbers long,
it's actually a very hard task to skim through an entire data
set to do that.
And you need some fast ways to do this.
This is purely algorithmic.
What is the vector that's the closest to mine in a given data
set?
There's no statistics going on.
But this is a building block of fast methods.
Low dimensional embedding.
If you want to visualize things, if you're like me,
you won't be able to visualize in dimensions much higher
than three.
And that's something that you need to do,
if you want to have-- you have extremely
high dimensional data, gene spaces, tens of thousands,
you want to project it down to something you can visualize.
You can not just do it in any random way,
otherwise you will lose all the structure of your data.
Spectral methods is something that's
a fairly algorithmic tool that's pervasive to all
of scientific computing I would say
in general, in particular in data
science has become extremely successful.
So if you want, there's some linear algebra.
It's not technically a prereq for this class.
But if you can take a class, if you're
interested in doing data science,
I encourage you to go to some numerical linear algebra,
linear algebra class, to understand what's going on.
We'll touch upon these when we talk about principal component
analysis.
Distributed optimization, emphasis on optimization.
This is when you have extremely large data sets.
You do not even store them on one single server.
How do you do this?
How do you combine them?
How do you do your optimization will be something
that's important to us.
And this is a huge field that's like part
of the algorithmic aspects of data science.
The statistical view sort of says,
let's assume we have infinite computational power.
And I want to squeeze as much as I can from my data.
OK.
When I say assume you have infinite computational power,
of course this will be something--
if you have the choice between two methods,
one is clearly going to be implementable,
the other one is clearly bound to fail in dimension larger
than three, you're probably going
to want to go for the one that makes more sense.
And so the way we think about the statistical view
is that data comes from a random process.
OK?
They're not just the sequence of numbers
that I need to feed into an algorithm.
They come from a random process, and they're actually
representative of this random process.
The goal of statistics is to extrapolate
from a finite number of data to what
you would get if you had an infinite amount
of observations.
When you have an infinite amount of data,
you basically can describe the random process
completely accurately.
So there's still some randomness, right?
If I pick-- even if I had observed extremely carefully
all of the students in the world over a period of 100 years,
I could not pick any one of you and start
making completely accurate guesses about who you are.
There's still some randomness in things
that I just do not understand.
By looking at you, I can not predict
things that are deep about you.
OK?
And we'll see how we model those things
that we can not understand from what we
observe into the random part.
Sometimes there will be true randomness-- randomly selected
things, throwing dice, picking cards.
Those are things where randomness arise naturally.
But usually statistical modeling says,
there's the part I understand from the data that I have,
and then there's everything I don't understand.
This I will pretend is random.
OK?
So if you understand the random process,
you will understand everything you
can understand from your data.
And this is basically the goal of statistics--
extrapolate from small number of data,
or even large to essentially potentially infinite.
And then once you know this, you know
you're going to be able to make the most accurate predictions,
for example.
So the science that understands randomness is probability.
Hence the prereq for this class.
So we will, in this first chapter,
do a very quick probability redux.
The goal is to make sure that we all speak the same language.
Because I will go fast on some aspects of probability.
Linearity of expectation, or how you compute a variance.
How do you use independence?
How do you do--
I don't know-- anything that you can think of that you
learn at the level of 18600.
I will forget somehow that this is probability,
and I will just assume that you know this
just like you understand when I speak English.
OK?
And so this is something you just
need to be comfortable with.
We will do a little bit just to make sure
that I press because this is a vast topic.
That was a whole semester of your life.
And so I just will talk a little bit
about putting emphasis so that we
are all on par about the things that
are slightly more esoteric within a probability class.
For example, different modes of convergence,
which is a very important notion in statistics-- almost sure
convergence, convergence in probability,
convergence in distribution, also known as weak convergence.
Those things we will see, because they're
important in statistics.
And I will just go through them once again,
to make sure we're all talking the same language.
But there's many things I won't.
So I just encourage you to brush up on your probability
if you feel you're a little rusty there.
OK.
So probability studies randomness.
And the kind of questions that you will see in probability
are when the random process is completely known.
So there is a random process, right?
There's something random.
And when you look at a probability exercise,
it usually says, here's the random process.
Here are the parameters of this random process.
And I'm going to ask you about what the outcomes look like.
What are likely outcomes?
What are unlikely outcomes?
What is the probability of a particular outcome?
This kind of thing.
OK.
So here's a typical example.
You roll one die.
Alice gets $1 if the number of dots is less than three.
Bob gets $2 if he gets at most two.
Do you want to be Alice or Bob?
So this is a question about expectation, right?
So this is a question about expectation.
And you know exactly what the expectations are, right?
So for example, if I look at the expectation for Bob--
for Alice, say-- so if I look at expectation for Alice,
this is the probability that she gets $1.
So what is the probability that she gets $1?
Yeah, 3/6, which is 1/2, times $1.
And so that's just a half dollar.
And so this is the expectation for Alice.
And the expectation for Bob is 1/3 times
$2, which is 0.66, 2/3 of $1.
OK.
So those are just expectation.
And these are the things that you might want to get.
So here implicitly, all of the data
that I gave you about the random process,
you sort of internalized by knowing that it's a die,
and what you know about a die is that there's
1/6 chances that it actually comes out
on the particular side.
OK.
And then there's rolling two dice.
This is a slightly more complex thing.
And if I ask you all these questions, what you're actually
going to be doing is you will internalize the idea
that the two dice are independent.
So you're always making modeling assumptions about your dice.
Maybe I didn't tell you that those dice are not
bolted together.
But you sort of assume that this is basically what's happening.
OK?
So this is modeling.
But in general, for those probability questions,
it's pretty clear what the probabilities of basic outcomes
are.
And then you can use the rules of probability,
like probability of union and intersection
and things like this, to start building probabilities
for more complex events.
So make sure that you know how to solve those two problems.